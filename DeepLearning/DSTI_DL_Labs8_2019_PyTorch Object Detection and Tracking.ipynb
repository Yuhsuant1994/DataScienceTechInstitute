{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python3.6",
      "language": "python",
      "name": "python3.6"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "DSTI-DL-Labs8-2019.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kbM62sa9YTD",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch Object Detection and Tracking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1gPvfEr-b6M",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Download the object detection system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30MyPAy_9aY2",
        "colab_type": "text"
      },
      "source": [
        "Download the code to run the object detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_0lszjBiRnZ",
        "colab_type": "code",
        "outputId": "beb63404-a198-4ed9-858f-fedaa3c68cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Install pycocotools\n",
        "git clone https://github.com/cfotache/pytorch_objectdetecttrack"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch_objectdetecttrack'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 36 (delta 0), reused 0 (delta 0), pack-reused 33\u001b[K\n",
            "Unpacking objects: 100% (36/36), done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK6676eiifkz",
        "colab_type": "code",
        "outputId": "0f5a00e1-12b9-4ffa-8ddf-d5d10c3f8c6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "%ls -l\n",
        "%cd pytorch_objectdetecttrack/\n",
        "%ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 532\n",
            "drwxr-xr-x 2 root root   4096 Oct 17 18:23 \u001b[0m\u001b[01;34mconfig\u001b[0m/\n",
            "-rw-r--r-- 1 root root  22960 Oct 29  2014 Harry_Potter_and_the_Order_of_the_Phoenix_poster.jpg\n",
            "drwxr-xr-x 2 root root   4096 Oct 17 18:38 \u001b[01;34mimages\u001b[0m/\n",
            "-rw-r--r-- 1 root root  14374 Oct 17 18:22 models.py\n",
            "-rw-r--r-- 1 root root   3735 Oct 17 18:22 object_tracker.py\n",
            "drwxr-xr-x 2 root root   4096 Oct 17 18:26 \u001b[01;34m__pycache__\u001b[0m/\n",
            "-rw-r--r-- 1 root root 451724 Oct 17 18:22 PyTorch_Object_Detection.ipynb\n",
            "drwxr-xr-x 6 root root   4096 Oct 17 19:59 \u001b[01;34mpytorch_objectdetecttrack\u001b[0m/\n",
            "-rw-r--r-- 1 root root   5192 Oct 17 18:22 PyTorch_Object_Tracking.ipynb\n",
            "-rw-r--r-- 1 root root    701 Oct 17 18:22 README.md\n",
            "-rw-r--r-- 1 root root  10271 Oct 17 18:22 sort.py\n",
            "drwxr-xr-x 3 root root   4096 Oct 17 18:22 \u001b[01;34mutils\u001b[0m/\n",
            "/content/pytorch_objectdetecttrack/pytorch_objectdetecttrack\n",
            "total 500\n",
            "drwxr-xr-x 2 root root   4096 Oct 17 19:59 \u001b[0m\u001b[01;34mconfig\u001b[0m/\n",
            "drwxr-xr-x 2 root root   4096 Oct 17 19:59 \u001b[01;34mimages\u001b[0m/\n",
            "-rw-r--r-- 1 root root  14374 Oct 17 19:59 models.py\n",
            "-rw-r--r-- 1 root root   3735 Oct 17 19:59 object_tracker.py\n",
            "-rw-r--r-- 1 root root 451724 Oct 17 19:59 PyTorch_Object_Detection.ipynb\n",
            "-rw-r--r-- 1 root root   5192 Oct 17 19:59 PyTorch_Object_Tracking.ipynb\n",
            "-rw-r--r-- 1 root root    701 Oct 17 19:59 README.md\n",
            "-rw-r--r-- 1 root root  10271 Oct 17 19:59 sort.py\n",
            "drwxr-xr-x 3 root root   4096 Oct 17 19:59 \u001b[01;34mutils\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi2oNph5-RTj",
        "colab_type": "text"
      },
      "source": [
        "Download the YoloV3 pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeAztsDgi2NO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell \n",
        "\n",
        "ls -l ./config\n",
        "cd ./config\n",
        "bash ./download_weights.sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdj92D2lHmrW",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Apply the object recognition on an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1QlbdQM-o7g",
        "colab_type": "text"
      },
      "source": [
        "Necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVrfDJWmiQXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from models import *\n",
        "from utils import *\n",
        "\n",
        "import os, sys, time, datetime, random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-88uqgJ2-uVg",
        "colab_type": "text"
      },
      "source": [
        "Define the configuration path and initialize Darknet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB6S46tX_pFl",
        "colab_type": "text"
      },
      "source": [
        "### Question: Where is defined Darknet? What is it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxtW_aVtOIVC",
        "colab_type": "text"
      },
      "source": [
        "### Question: What is the COCO dataset? What are the labels?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "TmYKQf5MiQXv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config_path='./config/yolov3.cfg'\n",
        "weights_path='./config/yolov3.weights'\n",
        "class_path='./config/coco.names'\n",
        "img_size=416\n",
        "conf_thres=0.8\n",
        "nms_thres=0.4\n",
        "\n",
        "# Load model and weights\n",
        "model = Darknet(config_path, img_size=img_size)\n",
        "model.load_weights(weights_path)\n",
        "model.cuda()\n",
        "model.eval()\n",
        "classes = utils.load_classes(class_path)\n",
        "Tensor = torch.cuda.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJCB8zh0Acpw",
        "colab_type": "text"
      },
      "source": [
        "Most of the following code deals with resizing the image to a square while maintaining its aspect ratio and padding the overflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooRM8fWOG1Ev",
        "colab_type": "text"
      },
      "source": [
        "### Question: What is the goal of the resizing step?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF1D3qoviQX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detect_image(img):\n",
        "    # scale and pad image\n",
        "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
        "    imw = round(img.size[0] * ratio)\n",
        "    imh = round(img.size[1] * ratio)\n",
        "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
        "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
        "                        (128,128,128)),\n",
        "         transforms.ToTensor(),\n",
        "         ])\n",
        "    \n",
        "    # convert image to Tensor\n",
        "    image_tensor = img_transforms(img).float()\n",
        "    print(image_tensor.shape)\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "    input_img = image_tensor.type(Tensor)\n",
        "    \n",
        "    # run inference on the model and get detections\n",
        "    with torch.no_grad():\n",
        "        detections = model(input_img)\n",
        "        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
        "    return detections[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR_67QikEZRX",
        "colab_type": "text"
      },
      "source": [
        "Print the directory with the image that can be tested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYZm3FT7iQX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls -l ./images\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xciTfH0YGjgF",
        "colab_type": "text"
      },
      "source": [
        "Load an arbitrary image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nszAgWcqFSWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%shell\n",
        "wget https://upload.wikimedia.org/wikipedia/en/e/e7/Harry_Potter_and_the_Order_of_the_Phoenix_poster.jpg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv8zv744CVG1",
        "colab_type": "text"
      },
      "source": [
        "The following cells load an image, get the detections, and then display it with the bounding boxes around detected objects. \n",
        "\n",
        "Most of the code deals with scaling and padding the image, as well as getting different colors for each detected class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wTiCznfiQX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load image and get detections\n",
        "#img_path = \"images/blueangels.jpg\"\n",
        "img_path = \"images/Intersection-Counts.jpg\"\n",
        "img_path = \"images/olympic-trials.jpg\"\n",
        "img_path = \"Harry_Potter_and_the_Order_of_the_Phoenix_poster.jpg\"\n",
        "prev_time = time.time()\n",
        "img = Image.open(img_path)\n",
        "detections = detect_image(img)\n",
        "inference_time = datetime.timedelta(seconds=time.time() - prev_time)\n",
        "print ('Inference Time: %s' % (inference_time))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO8R8_gkHUNu",
        "colab_type": "text"
      },
      "source": [
        "### Question: describe the content of \"detections\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLXyB1anEONk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(detections)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMadUvd9Fvam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SP8grHUmCnB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get bounding-box colors\n",
        "cmap = plt.get_cmap('tab20b') # get a colormap\n",
        "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
        "\n",
        "img = np.array(img)\n",
        "plt.figure()\n",
        "fig, ax = plt.subplots(1, figsize=(12,9))\n",
        "ax.imshow(img)\n",
        "\n",
        "pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
        "pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
        "unpad_h = img_size - pad_y\n",
        "unpad_w = img_size - pad_x\n",
        "\n",
        "if detections is not None:\n",
        "    unique_labels = detections[:, -1].cpu().unique()\n",
        "    n_cls_preds = len(unique_labels)\n",
        "    bbox_colors = random.sample(colors, n_cls_preds)\n",
        "    # browse detections and draw bounding boxes\n",
        "    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
        "        box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
        "        box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
        "        y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
        "        x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
        "        color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
        "        bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2, edgecolor=color, facecolor='none')\n",
        "        ax.add_patch(bbox)\n",
        "        plt.text(x1, y1, s=classes[int(cls_pred)], color='white', verticalalignment='top',\n",
        "                bbox={'color': color, 'pad': 0})\n",
        "plt.axis('off')\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMt4a6YeGACR",
        "colab_type": "text"
      },
      "source": [
        "### Question: Test an image you will choose yourself"
      ]
    }
  ]
}